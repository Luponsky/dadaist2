#!/usr/bin/env perl
#ABSTRACT: A program to run DADA2 from the CLI

# Minichangelog
# TODO 0.5.0  -  support for MicrobiomeAnalyst
# 0.4.0  - Improved verbose mode, added quality parameter, added support to save RDS, added check for dada2 installed, expanded test set, added makeSampleSheet
# 0.3.00 - Added a reference database downloader (dadaist2-getdb)
# 0.2.00 - Manual, save Rds, further analysis, taxonomy labels
# 0.1.09 - Added autocalculated trimming [qualified_wnd/qualified_min_qual]
# 0.1.07 - Added quality plot

use 5.012;
use warnings;
use Getopt::Long;
use Data::Dumper;
use FindBin qw($RealBin);
use File::Basename;
use File::Spec;
use File::Spec::Functions;
use File::Copy;
use Term::ANSIColor qw(color);
use FASTX::Reader;
use FASTX::ScriptHelper;
use JSON::PP;
use Digest::MD5 qw(md5_hex);
use File::Temp;
use Pod::Usage;

my $PROGRAM          = basename($0);
my $VERSION          = '0.4.00';
my $opt_verbose      = 0;
my $opt_debug        = 0;
my $opt_metadata     = undef;
my $opt_id_separator = '_';
my $opt_for_tag      = '_R1';
my $opt_rev_tag      = '_R2';
my $opt_seq_prefix   = 'ASV';
my $opt_temp_dir     = File::Spec->tmpdir();
my $opt_threads      = 2;
my $opt_tax_db       = "skip";
my $opt_save_rds     = 0;
my $save_rds         = "no";
my $opt_metadata_separator = "\t";

# FASTP Preprocess
# To be autocalculated:
my $fastp_trim_front1 = 14;
my $fastp_trim_front2 = 14;
my $fastp_trim_tail1  = 0;
my $fastp_trim_tail2  = 0;
my $fastp_minlen      = 110;

# DADA2
my $do_plots         = 'do_plots';
my $trunc_len_1      = 0;
my $trunc_len_2      = 0;
my $trunc_start_1    = 0;
my $trunc_start_2    = 0;
my $total_len_1      = 0;
my $total_len_2      = 0;
my $qualified_wnd    = 4;
my $qualified_min_qual = 28;

my $r1_len = 10_000;
my $r2_len = 10_000;
my $tot_reads = 0;
my ($opt_help,  $opt_log_filename, $opt_input_directory, $opt_output_directory, $opt_version, $opt_skip_plots);

GetOptions(
 'i|input-directory=s'             => \$opt_input_directory,
 'o|output-directory=s'            => \$opt_output_directory,
 'd|database=s'                    => \$opt_tax_db,
 'm|metadata=s'                    => \$opt_metadata,
 '1|for-tag=s'                     => \$opt_for_tag,
 '2|rev-tag=s'                     => \$opt_rev_tag,
 's|id-separator=s'                => \$opt_id_separator,
 'l|log-file=s'                    => \$opt_log_filename,
 't|threads=i'                     => \$opt_threads,
 'p|prefix=s'                      => \$opt_seq_prefix,
 'q|min-quality=f'                 => \$qualified_min_qual,
 'r|save-rds'                      => \$opt_save_rds,
 'tmp-dir=s'                       => \$opt_temp_dir,
 'skip-plots'                      => \$opt_skip_plots,
 'verbose'                         => \$opt_verbose,
 'debug'                           => \$opt_debug,
 'version'                         => \$opt_version,
 'h|help'                          => \$opt_help,
) || die " Parameters error, type --help for more info.\n";;

$opt_version && version();
$opt_help    && pod2usage({-exitval => 0, -verbose => 2});
$do_plots    = 'skip' if ($opt_skip_plots);

$opt_verbose = 1 if ($opt_debug);

if ($opt_save_rds) {
  $save_rds = 'save';
}

# Check essential parameters
my $fastp_threads = $opt_threads > 8 ? 8 : $opt_threads;
usage("Missing parameter(s): input directory (-i DIR).")  if (not defined $opt_input_directory);
usage("Missing parameter(s): output directory (-o DIR).") if (not defined $opt_output_directory);

if ( ! -d "$opt_input_directory") {
  die " FATAL ERROR: Input directory not found: $opt_input_directory\n"
}
if ( $opt_tax_db ne 'skip' and ! -e "$opt_tax_db") {
  die " FATAL ERROR: Taxonomy database not found: $opt_tax_db\n"
}

# Prepare temporary directories
my $temp_dir = File::Temp->newdir( 'dada2_XXXXXX',
		CLEANUP => 0,
		DIR => $opt_temp_dir);
my $dada2_temp = File::Spec->catdir("$temp_dir", 'dada2');

# Prepare output directory
if ( ! -d "$opt_output_directory") {
	mkdir "$opt_output_directory"    || die " FATAL ERROR:\n Unable to create directory $opt_output_directory/\n $!\n";
	mkdir File::Spec->catdir("$opt_output_directory", 'qc')    || die " FATAL ERROR:\n Unable to create directory $opt_output_directory/qc\n $!\n";

}




# Log file
my $log_filename =  $opt_log_filename // File::Spec->catfile($opt_output_directory, 'dadaist.log');
my $S = FASTX::ScriptHelper->new({
	verbose    => 1,
	logfile    => $log_filename,
});

$S->verbose("$PROGRAM $VERSION");
check_required_binaries({
	'fastp'   => 'fastp --version 2>&1',
	'RScript' => 'Rscript  --version 2>&1',
  'dada2 (lib)'  => 'echo "library(dada2)" | R --no-save > /dev/null',
	'fasttree'=> 'fasttree 2>&1 | head -n 1 | cut -c 11-',
	'clustalo'=> 'clustalo --version 2>&1',
#	'vsearch' => 'vsearch --version 2>&1 | head -n 1',
});

# Check taxonomy database
if ($opt_tax_db ne 'skip') {
	if (valid_gzipped_fasta($opt_tax_db)) {
		$S->verbose("Taxonomy database: $opt_tax_db");
	} else {
		$S->verbose("Taxonomy database not valid: $opt_tax_db. Taxonomy will be skipped.");
		$opt_tax_db = 'skip';
	}
}

# Metadata?
my $meta_samples;
if ($opt_metadata) {
  $meta_samples = check_metadata($opt_metadata);
  copy($opt_metadata,
		 File::Spec->catfile($opt_output_directory,'metadata.tsv')) || die " ERROR:\n Unable to copy metadÃ¹ file to $opt_output_directory.\n";

} else {
  $opt_metadata = File::Spec->catfile($opt_output_directory, 'metadata.tsv');
  my $meta_cmd = qq($RealBin/makeSampleSheet -i "$opt_input_directory" -o "$opt_metadata");
  $S->run($meta_cmd, {candie => 1});
  $meta_samples = check_metadata($opt_metadata);
}
$temp_dir = prepare_temporary_directories($temp_dir);

$S->verbose("Threads: $opt_threads");
$S->verbose("Output directory: $opt_output_directory");

my ($files_hash, $is_paired, $samples_count) = get_file_reads($opt_input_directory);
my $library_type = $is_paired > 0 ? 'paired-end' : 'single-end';
$S->verbose("Input directory \"$opt_input_directory\": $samples_count found ($library_type) ");


# Input sanitation with "fastp"
for my $sample (sort keys %{ $files_hash } ) {
	$S->verbose("Processing $sample");
	my $json_file = File::Spec->catfile($temp_dir, "${sample}.json");

	my $filt_cmd;
	if ($$files_hash{$sample}{rev}) {
		# Paired reads
		$filt_cmd = qq(fastp -w $fastp_threads -i "$$files_hash{$sample}{for}" -I "$$files_hash{$sample}{rev}" -o "$temp_dir"/for/${sample}_R1.fastq.gz -O  "$temp_dir"/rev/${sample}_R2.fastq.gz  ) .
			    qq( --trim_front1 $fastp_trim_front1 --trim_front2  $fastp_trim_front2 ).
			    qq( --detect_adapter_for_pe --disable_quality_filtering --n_base_limit  1  ).
			    qq( --json "$json_file" --html /dev/null ).
			    qq( --trim_tail1 $fastp_trim_tail1 --trim_tail2 $fastp_trim_tail2 --length_required $fastp_minlen );
	} else {
		# Single end
		$filt_cmd = qq(fastp -w $fastp_threads -i "$$files_hash{$sample}{for}" -o "$temp_dir"/for/${sample}_R1.fastq.gz   ) .
		    qq( --trim_front1 $fastp_trim_front1  ).
		    qq( --disable_quality_filtering --n_base_limit  1  ).
		    qq( --json "$json_file" --html /dev/null ).
		    qq( --trim_tail1 $fastp_trim_tail1 - --length_required $fastp_minlen );
		    die " FATAL ERROR: Single end processing is not supported at the moment.\n";
	}



	my $out = $S->run( $filt_cmd, { candie => 1 } );
	if ($out->{exit} != 0) {
		die " FATAL ERROR: 'fastp' failed processing sample $sample.\nCommand: $out->{cmd}\nOut: $out->{stdout}\nErr: $out->{stderr}";
	}
	copy($json_file, File::Spec->catdir("$opt_output_directory", 'qc')) or die " ERROR:\n Unable to copy JSON stats <$json_file> to <$opt_output_directory>.\n";
	my $summary = load_json_from_file( $json_file );
	my $raw_reads  = $summary->{summary}->{before_filtering}->{total_reads};
	my $pass_reads = $summary->{summary}->{after_filtering}->{total_reads};
	my $ins_size   = $summary->{insert_size}->{peak};
	my $q30        = $summary->{summary}->{after_filtering}->{q30_rate};
	my $passed     = 0;
	my @qual_R1    = @{ $summary->{read1_after_filtering}->{quality_curves}->{mean} };
	($trunc_start_1, $trunc_len_1, $total_len_1)  = get_qualified_positions(@qual_R1);
	my @qual_R2;
	if ($$files_hash{$sample}{rev}) {
		@qual_R2    = @{ $summary->{read2_after_filtering}->{quality_curves}->{mean} };
		($trunc_start_2, $trunc_len_2, $total_len_2)  = get_qualified_positions(@qual_R2);
	}


	$tot_reads += int( $pass_reads / 2);
	$r1_len     = $summary->{read1_after_filtering}->{total_cycles}
		if ($summary->{read1_after_filtering}->{total_cycles} < $r1_len);
	$r2_len     = $summary->{read2_after_filtering}->{total_cycles}
		if ($summary->{read2_after_filtering}->{total_cycles} < $r2_len);
	$passed += sprintf("%.2f", 100*$pass_reads/$raw_reads) if ($raw_reads);
	$S->verbose(qq($pass_reads/$raw_reads (${passed}%) reads kept.\n * Average insert size: $ins_size bp\n * Q30: $q30\n * Qualified region: [$trunc_start_1 - $trunc_len_1]/$total_len_1, [$trunc_start_2 - $trunc_len_2]/$total_len_2));
}

my $dada2_min_reads = $tot_reads < 10000 ? $tot_reads : 10000;
my @dada_paired_parameter_labels = (
'forward_reads', 'reverse_reads', 'feature_table_output', 'stats_output',
'filt_forward', 'filt_reverse', 'truncLenF', 'truncLenR',
'trimLeftF', 'trimLeftR', 'maxEEF', 'maxEER',
'truncQ', 'chimeraMethod', 'minFold','threads',
'nreads_learn','baseDir', 'doPlots', 'taxonomyDb', 'saveRDS'
);
my @dada_paired_args = (
# 1) File path to directory with the FORWARD .fastq.gz files to be processed.
#    Ex: path/to/dir/with/FWD_fastqgzs
	"${temp_dir}/for",
# 2) File path to directory with the REVERSE .fastq.gz files to be processed.
#    Ex: path/to/dir/with/REV_fastqgzs
	"${temp_dir}/rev",
# 3) File path to output tsv file. If already exists, will be overwritten.
#    Ex: path/to/output_file.tsv
	"${temp_dir}/dada2/dada2.tsv",
# 4) File path to tracking tsv file. If already exists, will be overwritte.
#    Ex: path/to/tracking_stats.tsv
	"${temp_dir}/dada2/stats.tsv",
# 5) File path to directory to write the filtered FORWARD .fastq.gz files. These files are intermediate
#               for the full workflow. Currently they remain after the script finishes. Directory must
#               already exist.
#    Ex: path/to/dir/with/FWD_fastqgzs/filtered
	"${temp_dir}/for/filtered",
# 6) File path to directory to write the filtered REVERSE .fastq.gz files. These files are intermediate
#               for the full workflow. Currently they remain after the script finishes. Directory must
#               already exist.
#    Ex: path/to/dir/with/REV_fastqgzs/filtered
	"${temp_dir}/rev/filtered",
### FILTERING ARGUMENTS ###
#
# 7) truncLenF - The position at which to truncate forward reads. Forward reads shorter
#               than truncLenF will be discarded.
#               Special values: 0 - no truncation or length filtering.
#    Ex: 240
    $trunc_len_1,
# 8) truncLenR - The position at which to truncate reverse reads. Reverse reads shorter
#               than truncLenR will be discarded.
#               Special values: 0 - no truncation or length filtering.
#    Ex: 160
    $trunc_len_2,
# 9) trimLeftF - The number of nucleotides to remove from the start of
#               each forward read. Should be less than truncLenF.
#    Ex: 0
    $trunc_start_1,
# 10) trimLeftR - The number of nucleotides to remove from the start of
#               each reverse read. Should be less than truncLenR.
#    Ex: 0
    $trunc_start_2,
# 11) maxEEF - Forward reads with expected errors higher than maxEEF are discarded.
#               Both forward and reverse reads are independently tested.
#    Ex: 2.0
    1.0,
# 12) maxEER - Reverse reads with expected errors higher than maxEER are discarded.
#               Both forward and reverse reads are independently tested.
#    Ex: 2.0
    1.4,
# 13) truncQ - Reads are truncated at the first instance of quality score truncQ.
#                If the read is then shorter than truncLen, it is discarded.
#    Ex: 2
    10,
### CHIMERA ARGUMENTS ###
#
# 14) chimeraMethod - The method used to remove chimeras. Valid options are:
#               none: No chimera removal is performed.
#               pooled: All reads are pooled prior to chimera detection.
#               consensus: Chimeras are detect in samples individually, and a consensus decision
#                           is made for each sequence variant.
#    Ex: consensus
    'consensus',
# 15) minParentFold - The minimum abundance of potential "parents" of a sequence being
#               tested as chimeric, expressed as a fold-change versus the abundance of the sequence being
#               tested. Values should be greater than or equal to 1 (i.e. parents should be more
#               abundant than the sequence being tested).
#    Ex: 1.0
    1.0,
### SPEED ARGUMENTS ###
#
# 16) nthreads - The number of threads to use.
#                 Special values: 0 - detect available and use all.
#    Ex: 1
     $opt_threads,
# 17) nreads_learn - The minimum number of reads to learn the error model from.
#                 Special values: 0 - Use all input reads.
#    Ex: 1000000
     $dada2_min_reads,

# 18)  dir (general use),
	${temp_dir},

# 19) make plots (if "do_plots" will make pdfs)
    ${do_plots},

# 20) Taxonomy database || 'skip'
	${opt_tax_db},

# 21) Save rds. 'save' => yes
	${save_rds},
);


## RUN DADA2
$S->verbose("Running DADA2...");
my $cmd_dada_paired = "Rscript --vanilla $RealBin/run_dada_paired.R " . join(" ", @dada_paired_args);

if ($opt_debug) {
  $S->verbose("Dada2 script parameters:");
  my $i_index = 0;
  for my $arg (@dada_paired_args) {
    my $label = $dada_paired_parameter_labels[$i_index];
    $i_index++;
    say STDERR  " * [$i_index]", color('blue'), " $label: " , color('reset'), "$arg";
  }
}
my $dada2_execution = $S->run($cmd_dada_paired, { candie => 1 });

# Check exit status of DADA2 wrapper
if ( $dada2_execution->{'exit'} != 0 ) {
  say STDERR color('red'), " *** ERROR *** ";
	$S->verbose("DADA2 Failed:\n> $dada2_execution->{stderr}\n> $dada2_execution->{stdout}");
	exit 1;
} else {
	$S->verbose("DADA2 output:\n" . color('yellow') . $dada2_execution->{stdout} . color('reset')) if ($opt_debug);
	$S->run(qq(rm -rf "$temp_dir"/for "$temp_dir"/rev))
}


# Copy output files

copy(File::Spec->catfile($dada2_temp, 'stats.tsv'),
	 File::Spec->catfile($opt_output_directory, 'dada2_stats.tsv')) || die " ERROR:\n Unable to copy stats.tsv file from $dada2_temp to $opt_output_directory.\n";

if (! $opt_skip_plots ) {
	copy(File::Spec->catfile("$temp_dir", 'quality_R1.pdf'),
		 File::Spec->catfile(File::Spec->catdir("$opt_output_directory", 'qc'), 'plot_R1.pdf')) || die " ERROR:\n Unable to copy plot_R1.pdf file from $temp_dir to $opt_output_directory.\n";
	copy(File::Spec->catfile("$temp_dir", 'quality_R2.pdf'),
		 File::Spec->catfile(File::Spec->catdir("$opt_output_directory", 'qc'), 'plot_R2.pdf')) || die " ERROR:\n Unable to copy plot_R2.pdf file from $temp_dir to $opt_output_directory.\n";

}

## PROCESS DADA2 MAIN OUTPUT
my $dada2_tsv   = File::Spec->catfile($dada2_temp, 'dada2.tsv');
my $dada2_file  = File::Spec->catfile($opt_output_directory, 'feature-table.tsv');
my $repseq_file = File::Spec->catfile($opt_output_directory, 'rep-seqs.fasta');

my @header = ();
open my $DADA_TSV,   '<', "$dada2_tsv"   || die " ERROR:\n Unable to open <$dada2_tsv>\n";
open my $dada2_FO,   '>', "$dada2_file"  || die " ERROR:\n Unable to write to <$dada2_file>\n";
open my $repseqs_FO, '>', "$repseq_file" || die " ERROR:\n Unable to write to <$repseq_file>\n";
my $feature_counter = 0;
$S->verbose("DADA2 Finished.");

# Create OTU Table and FASTA file from TSV
while ( my $line = readline($DADA_TSV) ) {
	if ($line=~/^#/) {
		$line=~s/_R1.fastq.gz//g;
		print {$dada2_FO} $line;
		chomp($line);
		@header = split /\t/, $line;
	} else {
		chomp($line);
		$feature_counter++;
		my ($sequence, @values) = split /\t/, $line;
		my $name;
		if ($opt_seq_prefix eq 'MD5') {
			$name = md5_hex($sequence);
		} else {
			$name = $opt_seq_prefix . $feature_counter;
		}

		say {$repseqs_FO} '>' , $name, "\n", $sequence;
		say {$dada2_FO} $name, "\t", join("\t", @values);
	}

}

my %output_files = (
 'feature-table' => "$dada2_file",
 'rep-seqs'      => "$repseq_file",
);


# Copy optional files
if ($opt_tax_db ne 'skip') {
  $output_files{'taxonomy-table'} = File::Spec->catfile("$opt_output_directory", 'taxonomy.txt');
	copy(File::Spec->catfile("$temp_dir", 'taxonomy.tsv'),
	     File::Spec->catfile("$opt_output_directory", 'taxonomy.txt')) || die " ERROR:\n Unable to copy taxonomy.txt file from " . File::Spec->catfile("$temp_dir", 'taxonomy.txt') . " to $opt_output_directory.\n";
}

if ($save_rds eq 'save') {
  $output_files{'feature-table-RDS'} = File::Spec->catfile("$opt_output_directory", 'feature-table.rds');
  copy(File::Spec->catfile("$dada2_temp", 'dada2.rds'),
	     File::Spec->catfile("$opt_output_directory", 'feature-table.rds')) || die " ERROR:\n Unable to copy RDS file from " . File::Spec->catfile("$temp_dir",  'dada2.rds') . " to $opt_output_directory.\n";

}

# Make Tree
my $msa_output  = File::Spec->catfile("$opt_output_directory", 'rep-seqs.msa');
my $tree_output = File::Spec->catfile("$opt_output_directory", 'rep-seqs.tree');
my $msa_cmd     = qq(clustalo -i "$repseq_file" -o "$msa_output" --outfmt=fasta --force);
my $tree_cmd    = qq(fasttree -nt -gtr -no2nd -spr 4 -quiet "$msa_output" > "$tree_output");
my $msa_exec    = $S->run($msa_cmd, { candie => 1 });
my $tree_exec   = $S->run($tree_cmd , { candie => 1 });
if ( $msa_exec->{'exit'} != 0 or $tree_exec->{'exit'} != 0) {
  say STDERR color('red'), " *** ERROR *** ";
	$S->verbose("Clustalo execution failed.\n cmd> $msa_cmd\noutput> $msa_exec->{stderr}\nstderr> $msa_exec->{stdout}");
	exit 1;
} elsif ( $tree_exec->{'exit'} != 0) {
  say STDERR color('red'), " *** ERROR *** ";
  $S->verbose("FastTree execution failed.\n cmd> $tree_cmd\noutput> $tree_exec->{stderr}\nstderr> $tree_exec->{stdout}");
	exit 1;
} else {
  $output_files{'multiple-alignment'} = $msa_output;
  $output_files{'features-tree'} = $tree_output;
}


# Delete, or not, temp dir
if ($opt_debug) {
  $S->verbose("Temporary directory _not_ deleted: $temp_dir");
} else {
  $S->verbose("Cleaning up");
  $S->run(qq(rm -rf "$temp_dir"));

}

# Print relevant output files produced
my $output_message = "Dadaist finished, output files saved:\n";
for my $f (keys %output_files) {
  $output_message .= " * $f: " . $output_files{$f} . "\n";
}
$S->verbose($output_message);



sub version {
	say "$PROGRAM v$VERSION";
	exit 0;
}

sub usage {
	say STDERR<<END;
USAGE:
 dadaist2 -i INPUT_DIR -o OUTPUT_DIR [-t TEMP_DIR]

 dadaist2 --help for full manual
END

 if ($_[0]) {
 	say "ERROR: $_[0]";
 	exit 1;
 }
}

sub check_metadata {
  my $file = shift @_;
  my $I;
  if (not open ($I, '<', $file)) {
    $S->verbose("Unable to load metadata from $file");
    exit 1;
  }
  my $count_lines = 0;
  my %samples = ();
  while (my $line = readline($I)) {
    $count_lines++;
    next if ($line =~/^#/);
    my @fields = split /$opt_metadata_separator/, $line;
    $samples{ $fields[0] }++;
  }
  return \%samples;
}

sub get_qualified_positions {
	# Scan a list of qualityes and return the boundaries of qualified quality
	my $len = @_;
	my $start = undef;
	my $end = undef;

	for (my $pos = 0; $pos < ($len - $qualified_wnd); $pos++) {
		my $avg = avg_array( @_[$pos..$pos+$qualified_wnd] );

		if ($avg > $qualified_min_qual) {
			$start = $pos unless defined $start;
		} else {
			next if not defined $start;
			$end = $pos unless defined $end;
		}


	}
	$end = $len unless defined $end;
	return($start, $end, $len);
}
sub avg_array {
	my $agg = 0;
	$agg += $_ for @_;
	return $agg/@_;
}
sub sum_array {
	my $agg = 0;
	$agg += $_ for @_;
	return $agg
}

sub load_json_from_file {
	my $file = shift @_;
	my $json_read = $S->run(qq(cat "$file"));


	my $data;
	eval {
		$data = decode_json $json_read->{stdout};
	};
	if ($@) {
		die " FATAL ERROR: Unable to decode JSON from <$file>:\n$@\n";
	}
	return $data;

}
sub get_file_reads {
	my ($dir) = @_;
	my $is_paired = undef;
	$dir = File::Spec->rel2abs($dir);
	my @files = <"$dir"/*.*>;
	my %samples;
	my $counter_for = 0;
	my $counter_rev = 0;
	for my $file (sort @files) {
		next if (substr($file, 0, 1) eq '.');
		my ($id) = split /$opt_id_separator/, basename($file);
		if ($file =~/$opt_for_tag/) {
			$counter_for++;
			$samples{$id}{'for'} = $file;
		} elsif ($file =~/$opt_rev_tag/) {
			$counter_rev++;
			$samples{$id}{'rev'} = $file;
		} else {
			$S->verbose("Skipping file <$file>: missing $opt_for_tag/$opt_rev_tag");
		}
	}
	if ($counter_for == 0 and $counter_rev == 0) {
		die "FATAL ERROR: No samples found in <$dir>.\n";
	} elsif ($counter_for == $counter_rev) {
		$is_paired = 1;
	} elsif ($counter_rev == 0 ) {
		$is_paired = 0;
	} else {
		die "FATAL ERROR: $counter_rev paired samples found, but $counter_for forward pairs found.\n";
	}
	return (\%samples, $is_paired, $counter_rev);
}

sub valid_gzipped_fasta {
	my $file = shift @_;
	my $reader;
	my $check = eval {
		$reader = FASTX::Reader->new({ filename => "$file"});

	};
	if ($@) {
		$S->verbose("Reference $file reading error: unable to open:\n$@");
		return 0;
	} else {
		if ($reader->{compressed} == 1) {
			return 1;
		} else {
			$S->verbose("Reference $file should be compressed with GZip!");
		}

	}

	exit;
}
sub check_required_binaries {

	for my $bin (sort keys  %{ $_[0]} ) {
		my $run = $S->run("${ $_[0]}{$bin}", { candie => 1});
		if ($run->{exit} == 0 and $run->{stdout} !~/not found/) {
			$S->verbose("Checking $bin: $run->{stdout}");
		} else {
			die "FATAL ERROR:\nDependency '$bin' not found or returned non zero exit value.\n";
		}

	}
}

sub prepare_temporary_directories {
 my $temp_dir = shift @_;
 $temp_dir = File::Spec->rel2abs($temp_dir);
	mkdir "$temp_dir/for"            || die " FATAL ERROR:\n Unable to create directory $temp_dir/for\n $!\n";
	mkdir "$temp_dir/rev"            || die " FATAL ERROR:\n Unable to create directory $temp_dir/rev\n $!\n";
	mkdir "$temp_dir/for/filtered"   || die " FATAL ERROR:\n Unable to create directory $temp_dir/for/filtered\n $!\n";
	mkdir "$temp_dir/rev/filtered"   || die " FATAL ERROR:\n Unable to create directory $temp_dir/rev/filtered\n $!\n";
	mkdir "$dada2_temp"              || die " FATAL ERROR:\n Unable to create directory $temp_dir/dada2\n $!\n";
  $S->verbose("Temporary directory: $temp_dir");
  return $temp_dir;
}

__END__

=head1 NAME

B<dadaist2> - a shell wrapper for DADA2, to detect representative sequences and generate
a feature table starting from Illumina Paired End reads.

=head1 AUTHOR

Andrea Telatin <andrea.telatin@quadram.ac.uk>

=head1 SYNOPSIS

dadaist2 [options] -i INPUT_DIR -o OUTPUT_DIR

=head1 PARAMETERS

=over 4

=item I<-i>, I<--input-directory> DIRECTORY

Directory containing the paired end files in FASTQ format, gzipped or not.

=item I<-o>, I<--output-directory> DIRECTORY

Output directory (will be created).

=item I<-m>, I<--metadata> FILE

Metadata file in TSV format, first column must match sample IDs. If not supplied
a template will be autogenerated using C<makeSampleSheet>.

=item I<-d>, I<--database> DATABASE

Reference database in gzipped FASTA format, specify 'skip' not to assign
taxonomy (default: skip)

=item I<-q>, I<--min-qual> FLOAT

Minimum average quality for DADA2 truncation (default: 28)

=item I<-r>, I<--save-rds>

Save a copy of the RDS file (default: off)

=item I<-1>, I<--for-tag> STRING [and I<-2>, I<--rev-tag>]

String identifying the forward (and reverse) pairs. Default are _R1 and _R2.

=item I<-s>, I<--id-separator> STRING

String used to separate the "sample name" from the filename. Default "_".

=item I<-p>, I<--prefix> STRING

Prefix for the output FASTA file, if "MD5" is specified, the sequence MD5 hash
will be used instead. Default is "ASV".

=item I<-l>, I<--log-file> FILE

Filename for the program log.

=back

=head1 SOURCE CODE AND DOCUMENTATION

The program is freely available at https://github.com/quadram-institute-bioscience/dadaist2
released under the MIT licence. The website contains further DOCUMENTATION.
