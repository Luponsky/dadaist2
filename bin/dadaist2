#!/usr/bin/env perl
#ABSTRACT: A program to run DADA2 from the CLI
use 5.012;
use warnings;
my $VERSION          = '0.7.2';

=pod

=head1 NAME

B<dadaist2> - a shell wrapper for DADA2, to detect representative sequences and generate
a feature table starting from Illumina Paired End reads.

=head1 AUTHOR

Andrea Telatin <andrea.telatin@quadram.ac.uk>

=head1 SYNOPSIS

  dadaist2 [options] -i INPUT_DIR -o OUTPUT_DIR

=cut

# Minichangelog
# 0.7.2  - No join
# 0.7.0  - Decipher support
# 0.6.0  - Phyloseq
# 0.5.0  - support for MicrobiomeAnalyst (enabled by default)
# 0.4.0  - Improved verbose mode, added quality parameter, added support to save RDS, added check for dada2 installed, expanded test set, added makeSampleSheet
# 0.3.00 - Added a reference database downloader (dadaist2-getdb)
# 0.2.00 - Manual, save Rds, further analysis, taxonomy labels
# 0.1.09 - Added autocalculated trimming [qualified_wnd/qualified_min_qual]
# 0.1.07 - Added quality plot



use Getopt::Long;
use Data::Dumper;
use FindBin qw($RealBin);
use File::Basename;
use File::Spec;
use File::Spec::Functions;
use File::Copy;
use Term::ANSIColor qw(color);
use FASTX::Reader;
use FASTX::ScriptHelper;
use JSON::PP;
use Digest::MD5 qw(md5_hex);
use File::Temp;
use Pod::Usage;

my $PROGRAM          = basename($0);
my $opt_verbose      = 1;
my $opt_quiet        = 0;
my $opt_debug        = 0;
my $opt_metadata     = undef;
my $opt_id_separator = '_';
my $opt_for_tag      = '_R1';
my $opt_rev_tag      = '_R2';
my $opt_seq_prefix   = 'ASV';
my $opt_temp_dir     = File::Spec->tmpdir();
my $opt_threads      = 2;
my $opt_tax_db       = "skip";
my $decipher_db      = undef;
my $opt_save_rds     = 0;
my $save_rds         = "no";
my $opt_metadata_separator = "\t";
my $opt_notify       = 0;
# FASTP Preprocess
# To be autocalculated:
my $fastp_trim_front1 = 20;
my $fastp_trim_front2 = 20;
my $fastp_trim_tail1  = 0;
my $fastp_trim_tail2  = 0;
my $fastp_minlen      = 110;

# AmpliCheck
my $opt_primers       = undef;
my $primer_for        = undef;
my $primer_rev        = undef;

# DADA2
my $do_plots         = 'do_plots';
my $trunc_len_1      = 0;
my $trunc_len_2      = 0;
my $trunc_start_1    = 0;
my $trunc_start_2    = 0;
my $opt_maxee_1      = 1.0;
my $opt_maxee_2      = 1.5;
my $total_len_1      = 0;
my $total_len_2      = 0;
my $qualified_wnd    = 4;
my $qualified_min_qual = 28;
my $opt_no_trunc;
my $opt_justconcat = 0;

my @taxonomy = ();
my $r1_len = 10_000;
my $r2_len = 10_000;
my $tot_reads = 0;
my ($opt_help,  $opt_log_filename, $opt_input_directory, $opt_output_directory, $opt_version, $opt_skip_plots);

my $chooser = {
	'filtering' => 'fastp',
	'database'  => undef,
};

# Check dependencies
my %dependencies = (
	'fastp'   => 'fastp --version 2>&1',
	'fu-primers' => 'fu-primers --version 2>&1',
	'RScript' => 'Rscript  --version 2>&1',
  'dada2 (lib)'  => 'echo "library(dada2)" | R --no-save > /dev/null',
	'fasttree'=> 'fasttree 2>&1 | head -n 1 | cut -c 11-',
	'clustalo'=> 'clustalo --version 2>&1',
  'exporter' => File::Spec->catfile($RealBin, 'dadaist2-exporter') . ' --version',
);

my $parsed = GetOptions(
 'i|input-directory=s'             => \$opt_input_directory,
 'o|output-directory=s'            => \$opt_output_directory,
 'd|database=s'                    => \$opt_tax_db,
 'm|metadata=s'                    => \$opt_metadata,
 '1|for-tag=s'                     => \$opt_for_tag,
 '2|rev-tag=s'                     => \$opt_rev_tag,
 's|id-separator=s'                => \$opt_id_separator,
 'l|log-file=s'                    => \$opt_log_filename,
 't|threads=i'                     => \$opt_threads,
 'p|prefix=s'                      => \$opt_seq_prefix,
 'q|min-quality=f'                 => \$qualified_min_qual,
 'r|save-rds'                      => \$opt_save_rds,
 's1|trim-primer-for=i'            => \$fastp_trim_front1,
 's2|trim-primer-rev=i'            => \$fastp_trim_front2,
 'j|just-concat'                   => \$opt_justconcat,
 'primers=s'                       => \$opt_primers,
 'maxee1=f'                        => \$opt_maxee_1,
 'maxee2=f'                        => \$opt_maxee_2,
 'no-trunc'                        => \$opt_no_trunc,
 'tmp-dir=s'                       => \$opt_temp_dir,
 'skip-plots'                      => \$opt_skip_plots,
 'quiet'                           => \$opt_quiet,
 'verbose'                         => \$opt_verbose,
 'debug'                           => \$opt_debug,
 'popup'                           => \$opt_notify,
 'version'                         => \$opt_version,
 'h|help'                          => \$opt_help,
) || die " Parameters error, type --help for more info.\n";;

die " ERROR: Unexpected parameters: ", join(", ", @ARGV), "\n" if ($ARGV[0]);
=pod

=head1 PARAMETERS

=head2 Main Parameters

=over 4

=item I<-i>, I<--input-directory> DIRECTORY

Directory containing the paired end files in FASTQ format, gzipped or not.

=item I<-o>, I<--output-directory> DIRECTORY

Output directory (will be created).

=item I<-m>, I<--metadata> FILE

Metadata file in TSV format, first column must match sample IDs. If not supplied
a template will be autogenerated using C<makeSampleSheet>.

=item I<-d>, I<--database> DATABASE

Reference database in gzipped FASTA format, specify 'skip' not to assign
taxonomy (default: skip)

=item I<-t>, I<--threads> INT

Number of threads (default: 2)

=item I<--primers> FOR:REV

Strip primers with amplicheck, otherwise trim the primer regions
with fastp.

=item I<-j>, I<--just-concat>

Do not try merging paired end reads, just concatenate.

=back


=head2 Metabarcoding processing

=over 4

=item I<-q>, I<--min-qual> FLOAT

Minimum average quality for DADA2 truncation (default: 28)


=item I<--no-trunc>

Do not truncate reads (required for non-overlapping amplicons, like ITS)

=item I<--maxee1>, and I<--maxee2> FLOAT

Maximum Expected Errors in R1 and R2, respectively (default: 1.0 and 1.5)

=item I<s1>, I<--trim-primer-for> INT

Trim primer from R1 read specifying the number of bases. Similarly
use C<-s2> (C<--trim-primer-rev>) to remove the front bases from the
reverse pair (R2). Default: 20 bases each side.

=item I<-r>, I<--save-rds>

Save a copy of the RDS file (default: off)

=item I<-1>, I<--for-tag> STRING [and I<-2>, I<--rev-tag>]

String identifying the forward (and reverse) pairs. Default are _R1 and _R2.

=back

=head2 Other parameters

=over 4

=item I<-s>, I<--id-separator> STRING

String used to separate the "sample name" from the filename. Default "_".

=item I<-p>, I<--prefix> STRING

Prefix for the output FASTA file, if "MD5" is specified, the sequence MD5 hash
will be used instead. Default is "ASV".

=item I<-l>, I<--log-file> FILE

Filename for the program log.

=item I<--popup>

Display popup notifications (tested on MacOS and Ubuntu)

=back


=cut
$opt_verbose = 0 if ($opt_quiet);
$opt_version && version();
$opt_help    && pod2usage({-exitval => 0, -verbose => 2});
$do_plots    = 'skip' if ($opt_skip_plots);

$opt_verbose = 1 if ($opt_debug);

if ($opt_primers) {
	($primer_for, $primer_rev) = split /:/, $opt_primers;
	die " ERROR: Primers should be in the format --primers FORWARD:REVERSE. Semicolon not found.\n" 
	  unless defined $primer_rev;
  $chooser->{'filtering'}  = 'seqfu';
}
if ($opt_save_rds) {
  $save_rds = 'save';
}

# Check essential parameters
my $fastp_threads = $opt_threads > 8 ? 8 : $opt_threads;
usage("Missing parameter(s): input directory (-i DIR).")  if (not defined $opt_input_directory);
usage("Missing parameter(s): output directory (-o DIR).") if (not defined $opt_output_directory);


say STDERR color('bold magenta'), " Dadaist2 ", color('reset')
 if not $opt_quiet;

if ( ! -d "$opt_input_directory") {
  die " FATAL ERROR: Input directory not found: $opt_input_directory\n"
}
if ( $opt_tax_db ne 'skip' and ! -e "$opt_tax_db") {
  die " FATAL ERROR: Taxonomy database not found: $opt_tax_db\n"
}

# Prepare temporary directories
my $temp_dir = File::Temp->newdir( 'dadaist2_XXXXXX',
		CLEANUP => 0,
		DIR => $opt_temp_dir);
my $dada2_temp = File::Spec->catdir("$temp_dir", 'dada2');

# Prepare output directory
$opt_output_directory = File::Spec->rel2abs($opt_output_directory) . "/";

$opt_output_directory .= '/' if ($opt_output_directory !~/\/$/);
if ( ! -d "$opt_output_directory") {
	mkdir "$opt_output_directory"    || die " FATAL ERROR:\n Unable to create directory $opt_output_directory/\n $!\n";
}
mkdir File::Spec->catdir("$opt_output_directory", 'qc')    || die " FATAL ERROR:\n Unable to create directory $opt_output_directory/qc\n $!\n";
die "ERROR: Unable to make ", File::Spec->catdir("$opt_output_directory", 'qc'), " directory.\n" unless (-d File::Spec->catdir("$opt_output_directory", 'qc') );



# Log file
my $log_filename =  $opt_log_filename // File::Spec->catfile($opt_output_directory, 'dadaist.log');
my $S = FASTX::ScriptHelper->new({
	verbose    => $opt_verbose,
	logfile    => $log_filename,
});

$S->verbose("$PROGRAM $VERSION");
notify_message("Starting dadaist2");

# If metadata is not supplied, check Metadata maker
if (! $opt_metadata) {
  $dependencies{'makeSampleSheet'} = File::Spec->catfile($RealBin, 'makeSampleSheet') . ' --version';
}

# Check taxonomy database
if ($opt_tax_db ne 'skip') {
	if ($opt_tax_db =~/RData$/i ) {
		$chooser->{'database'} = 'decipher';
 		# This is a Decipher database
		$dependencies{'DECIPHER'} = 'echo "library(DECIPHER)" | R --no-save > /dev/null';
		$dependencies{'DECIPHER-Run'} = "$RealBin/dadaist2-assigntax --version";
		$decipher_db = $opt_tax_db;
		$opt_tax_db = 'skip';
		$S->verbose("DECIPHER Taxonomy database found: $decipher_db");
	} else {
		# This is a DADA2 Naive classifier database
		if (valid_gzipped_fasta($opt_tax_db)) {
			$chooser->{'database'} = 'dada2';
			$dependencies{'Taxonomy'} = File::Spec->catfile($RealBin, 'dadaist2-assigntax') . ' --version';
			$S->verbose("Taxonomy database found: $opt_tax_db");
		} else {
			$S->verbose("Taxonomy database not valid: $opt_tax_db. Taxonomy will be skipped.");
			$opt_tax_db = 'skip';
		}
	}
	
} else {
	$S->verbose("No taxonomy database specified: skipping.");
}


# Dependency check

check_required_binaries(\%dependencies);

# Metadata?
my $meta_samples;
if ($opt_metadata) {
  $meta_samples = check_metadata($opt_metadata);
  copy($opt_metadata,
		 File::Spec->catfile($opt_output_directory,'metadata.tsv')) || die " ERROR:\n Unable to copy metadù file to $opt_output_directory.\n";

} else {
  $opt_metadata = File::Spec->catfile($opt_output_directory, 'metadata.tsv');
  my $meta_cmd = qq($RealBin/makeSampleSheet -i "$opt_input_directory" -o "$opt_metadata");
  $S->run($meta_cmd, {candie => 1});
  $meta_samples = check_metadata($opt_metadata);
}
$temp_dir = prepare_temporary_directories($temp_dir);

$S->verbose("Threads: $opt_threads");
$S->verbose("Output directory: $opt_output_directory");

my ($files_hash, $is_paired, $samples_count) = get_file_reads($opt_input_directory);
my $library_type = $is_paired > 0 ? 'paired-end' : 'single-end';
$S->verbose("Input directory \"$opt_input_directory\": $samples_count found ($library_type) ");


# Input sanitation with "fastp"
my @failed_samples = ();
my $counter_samples = 0;
my $tot = scalar keys %{ $files_hash };
for my $sample (sort keys %{ $files_hash } ) {
	$counter_samples++;
	$S->verbose("Processing $sample ($counter_samples/$tot)");
	my $json_file = File::Spec->catfile($temp_dir, "${sample}.json");
	my $filt_cmd;
	my $out;

	if ( $chooser->{'filtering'} eq 'fastp' and not defined $primer_for) {
		if ($$files_hash{$sample}{rev}) {
			# Paired reads
			$filt_cmd = qq(fastp -w $fastp_threads -i "$$files_hash{$sample}{for}" -I "$$files_hash{$sample}{rev}" -o "$temp_dir"/for/${sample}_R1.fastq.gz -O  "$temp_dir"/rev/${sample}_R2.fastq.gz  ) .
						qq( --trim_front1 $fastp_trim_front1 --trim_front2  $fastp_trim_front2 ).
						qq( --detect_adapter_for_pe --disable_quality_filtering --n_base_limit  1  ).
						qq( --json "$json_file" --html /dev/null ).
						qq( --trim_tail1 $fastp_trim_tail1 --trim_tail2 $fastp_trim_tail2 --length_required $fastp_minlen );
		} else {
			# Single end
			$filt_cmd = qq(fastp -w $fastp_threads -i "$$files_hash{$sample}{for}" -o "$temp_dir"/for/${sample}_R1.fastq.gz   ) .
					qq( --trim_front1 $fastp_trim_front1  ).
					qq( --disable_quality_filtering --n_base_limit  1  ).
					qq( --json "$json_file" --html /dev/null ).
					qq( --trim_tail1 $fastp_trim_tail1 - --length_required $fastp_minlen );
					die " FATAL ERROR: Single end processing is not supported at the moment.\n";
		}

		# Run FASTP
		$out = $S->run( $filt_cmd, { candie => 1 } );

	# Use Amplicheck (SeqFu) 
	} elsif ($chooser->{'filtering'} eq 'seqfu' and defined $primer_for) {
		if ($$files_hash{$sample}{rev}) {
			$filt_cmd = qq(fu-primers -1 "$$files_hash{$sample}{for}" -2 "$$files_hash{$sample}{rev}" ).
									qq( --primer-for "$primer_for" --primer-rev "$primer_rev" | seqfu deinterleave --for-ext "_R1.fastq" --rev-ext "_R2.fastq" -o "$temp_dir"/for/${sample} -);
			$S->verbose($filt_cmd) if ($opt_debug);
			$out = $S->run( $filt_cmd, { candie => 1 } );
			$S->run(qq(gzip "$temp_dir"/for/${sample}*));
			move(
				File::Spec->catfile(
					File::Spec->catdir($temp_dir, 'for'),
					"${sample}_R2.fastq.gz"),
				File::Spec->catdir($temp_dir, 'rev')
			);
		} else {
			die " Single end processing not supported at the moment.\n";
		}
	} elsif ( $chooser->{'filtering'} eq 'skip' ) {
		copy(
				$$files_hash{$sample}{for},
				File::Spec->catdir($temp_dir, 'for')
			);
		copy(
				$$files_hash{$sample}{rev},
				File::Spec->catdir($temp_dir, 'rev')
			);
	} else {
		$S->verbose("Unexpected setting: filtering should be performed either via trimming (fastp) or primer removal (SeqFu).\n");
		die;
	}

	
	if ($out->{exit} != 0) {
		die " FATAL ERROR: 'fastp' failed processing sample $sample.\nCommand: $out->{cmd}\nOut: $out->{stdout}\nErr: $out->{stderr}";
	}

  if ( $chooser->{'filtering'} eq 'fastp' ) {
		copy($json_file, 
			File::Spec->catdir("$opt_output_directory", 'qc/')) or 
			die " ERROR:\n Unable to copy JSON stats <$json_file> to <$opt_output_directory>.\n";
		my $summary = load_json_from_file( $json_file );
		my $raw_reads  = $summary->{summary}->{before_filtering}->{total_reads};
		my $pass_reads = $summary->{summary}->{after_filtering}->{total_reads};
		my $ins_size   = $summary->{insert_size}->{peak};
		my $q30        = $summary->{summary}->{after_filtering}->{q30_rate};
		my $passed     = 0;
		my @qual_R1    = @{ $summary->{read1_after_filtering}->{quality_curves}->{mean} };
		my $fastp_qc_pass = 1;
		($trunc_start_1, $trunc_len_1, $total_len_1)  = get_qualified_positions(@qual_R1);
		$fastp_qc_pass = 0 unless (defined $trunc_start_1);

		my @qual_R2;
		if ($$files_hash{$sample}{rev}) {
			@qual_R2    = @{ $summary->{read2_after_filtering}->{quality_curves}->{mean} };
			($trunc_start_2, $trunc_len_2, $total_len_2)  = get_qualified_positions(@qual_R2);
			$fastp_qc_pass = 0 unless (defined $trunc_start_2);
		}

		if (not $fastp_qc_pass) {
			$S->verbose("Sample failed QC: " . $sample);
			push(@failed_samples, $sample);
		}


		$tot_reads += int( $pass_reads / 2);
		$r1_len     = $summary->{read1_after_filtering}->{total_cycles}
			if ($summary->{read1_after_filtering}->{total_cycles} < $r1_len);
		$r2_len     = $summary->{read2_after_filtering}->{total_cycles}
			if ($summary->{read2_after_filtering}->{total_cycles} < $r2_len);
		$passed += sprintf("%.2f", 100*$pass_reads/$raw_reads) if ($raw_reads);
		$S->verbose(qq($pass_reads/$raw_reads (${passed}%) reads kept.\n * Average insert size: $ins_size bp\n * Q30: $q30\n * Qualified region: [$trunc_start_1 - $trunc_len_1]/$total_len_1, [$trunc_start_2 - $trunc_len_2]/$total_len_2));

	} elsif ($chooser->{'filtering'} = 'seqfu' ) {
		$trunc_start_1 = 0;
		$trunc_len_1 = 0;
		$trunc_start_2 = 0;
		$trunc_len_2 = 0;
		$S->verbose($out->{stderr});
	}
}

$S->verbose(scalar @failed_samples . " failed samples: " . join(",", @failed_samples))
  if (scalar @failed_samples);

notify_message("Read trimming finished, starting DADA2.");

my $dada2_min_reads = $tot_reads < 10000 ? $tot_reads : 10000;
my @dada_paired_parameter_labels = (
'forward_reads', 'reverse_reads', 'feature_table_output', 'stats_output',
'filt_forward', 'filt_reverse', 'truncLenF', 'truncLenR',
'trimLeftF', 'trimLeftR', 'maxEEF', 'maxEER',
'truncQ', 'chimeraMethod', 'minFold','threads',
'nreads_learn','baseDir', 'doPlots', 'taxonomyDb', 'saveRDS', 'noMerge'
);


if  ($opt_no_trunc) {
	$S->verbose("Skipping truncation");
	$trunc_len_1 = 0;
	$trunc_len_2 = 0;
}
my @dada_paired_args = (
# 1) File path to directory with the FORWARD .fastq.gz files to be processed.
#    Ex: path/to/dir/with/FWD_fastqgzs
	"${temp_dir}/for",

# 2) File path to directory with the REVERSE .fastq.gz files to be processed.
#    Ex: path/to/dir/with/REV_fastqgzs
	"${temp_dir}/rev",

# 3) File path to output tsv file. If already exists, will be overwritten.
#    Ex: path/to/output_file.tsv
	"${temp_dir}/dada2/dada2.tsv",

# 4) File path to tracking tsv file. If already exists, will be overwritte.
#    Ex: path/to/tracking_stats.tsv
	"${temp_dir}/dada2/stats.tsv",

# 5) File path to directory to write the filtered FORWARD .fastq.gz files. These files are intermediate
#               for the full workflow. Currently they remain after the script finishes. Directory must
#               already exist.
#    Ex: path/to/dir/with/FWD_fastqgzs/filtered
	"${temp_dir}/for/filtered",

# 6) File path to directory to write the filtered REVERSE .fastq.gz files. These files are intermediate
#               for the full workflow. Currently they remain after the script finishes. Directory must
#               already exist.
#    Ex: path/to/dir/with/REV_fastqgzs/filtered
	"${temp_dir}/rev/filtered",


### FILTERING ARGUMENTS ###
#
# 7) truncLenF - The position at which to truncate forward reads. Forward reads shorter
#               than truncLenF will be discarded.
#               Special values: 0 - no truncation or length filtering.
#    Ex: 240
    $trunc_len_1,

# 8) truncLenR - The position at which to truncate reverse reads. Reverse reads shorter
#               than truncLenR will be discarded.
#               Special values: 0 - no truncation or length filtering.
#    Ex: 160
    $trunc_len_2,

# 9) trimLeftF - The number of nucleotides to remove from the start of
#               each forward read. Should be less than truncLenF.
#    Ex: 0
    $trunc_start_1,

# 10) trimLeftR - The number of nucleotides to remove from the start of
#               each reverse read. Should be less than truncLenR.
#    Ex: 0
    $trunc_start_2,

# 11) maxEEF - Forward reads with expected errors higher than maxEEF are discarded.
#               Both forward and reverse reads are independently tested.
#    Ex: 2.0
    $opt_maxee_1,

# 12) maxEER - Reverse reads with expected errors higher than maxEER are discarded.
#               Both forward and reverse reads are independently tested.
#    Ex: 2.0
    $opt_maxee_2,

# 13) truncQ - Reads are truncated at the first instance of quality score truncQ.
#                If the read is then shorter than truncLen, it is discarded.
#    Ex: 2
    10,
### CHIMERA ARGUMENTS ###
#
# 14) chimeraMethod - The method used to remove chimeras. Valid options are:
#               none: No chimera removal is performed.
#               pooled: All reads are pooled prior to chimera detection.
#               consensus: Chimeras are detect in samples individually, and a consensus decision
#                           is made for each sequence variant.
#    Ex: consensus
    'consensus',
# 15) minParentFold - The minimum abundance of potential "parents" of a sequence being
#               tested as chimeric, expressed as a fold-change versus the abundance of the sequence being
#               tested. Values should be greater than or equal to 1 (i.e. parents should be more
#               abundant than the sequence being tested).
#    Ex: 1.0
    1.0,
### SPEED ARGUMENTS ###
#
# 16) nthreads - The number of threads to use.
#                 Special values: 0 - detect available and use all.
#    Ex: 1
     $opt_threads,
# 17) nreads_learn - The minimum number of reads to learn the error model from.
#                 Special values: 0 - Use all input reads.
#    Ex: 1000000
     $dada2_min_reads,

# 18)  dir (general use),
	${temp_dir},

# 19) make plots (if "do_plots" will make pdfs)
    ${do_plots},

# 20) Taxonomy database || 'skip'
	${opt_tax_db},

# 21) Save rds. 'save' => yes
	${save_rds},

# 22) Just concat 0=false
    ${opt_justconcat}
);


## RUN DADA2
$S->verbose("Running DADA2...");
my $cmd_dada_paired = "Rscript --vanilla $RealBin/D2-dada.R " . join(" ", @dada_paired_args);

$S->verbose($cmd_dada_paired) if ($opt_debug);
if ($opt_debug) {
  $S->verbose("Dada2 script parameters:");
  my $i_index = 0;
  for my $arg (@dada_paired_args) {
    my $label = $dada_paired_parameter_labels[$i_index];
    $i_index++;
    say STDERR  " * [$i_index]", color('blue'), " $label: " , color('reset'), "$arg";
  }
}
my $dada2_execution = $S->run($cmd_dada_paired, { candie => 1 });
notify_message("DADA2 finished. Finalizing...");


# Check exit status of DADA2 wrapper
if ( $dada2_execution->{'exit'} != 0 ) {
  say STDERR color('red'), " *** ERROR *** " if ($opt_verbose);
	$S->verbose("DADA2 Failed:\n> $dada2_execution->{stderr}\n> $dada2_execution->{stdout}");
	exit 1;
} else {
	$S->verbose("DADA2 output:\n" . color('yellow') . $dada2_execution->{stdout} . color('reset')) if ($opt_debug);
	$S->run(qq(rm -rf "$temp_dir"/for "$temp_dir"/rev))
}


# Copy output files

copy(File::Spec->catfile($dada2_temp, 'stats.tsv'),
	 File::Spec->catfile($opt_output_directory, 'dada2_stats.tsv')) || die " ERROR:\n Unable to copy stats.tsv file from $dada2_temp to $opt_output_directory.\n";

if (! $opt_skip_plots ) {
	copy(File::Spec->catfile("$temp_dir", 'quality_R1.pdf'),
		 File::Spec->catfile(File::Spec->catdir("$opt_output_directory", 'qc'), 'quality_R1.pdf')) || 
		 	die " ERROR:\n Unable to copy quality_R1.pdf file from <$temp_dir> to <$opt_output_directory>.\n";
	copy(File::Spec->catfile("$temp_dir", 'quality_R2.pdf'),
		 File::Spec->catfile(File::Spec->catdir("$opt_output_directory", 'qc'), 'quality_R2.pdf')) || 
		 	die " ERROR:\n Unable to copy quality_R2.pdf file from <$temp_dir> to <$opt_output_directory>.\n";

}

## PROCESS DADA2 MAIN OUTPUT
my $dada2_tsv   = File::Spec->catfile($dada2_temp, 'dada2.tsv');
my $dada2_file  = File::Spec->catfile($opt_output_directory, 'feature-table.tsv');
my $repseq_file = File::Spec->catfile($opt_output_directory, 'rep-seqs.fasta');

copy($dada2_tsv,
	 File::Spec->catfile($opt_output_directory, 'dada2_raw.tsv')) || 
	 die " ERROR:\n Unable to copy feature table file from $dada2_temp to $opt_output_directory.\n";

my @header = ();
open my $DADA_TSV,   '<', "$dada2_tsv"   || die " ERROR:\n Unable to open <$dada2_tsv>\n";
open my $dada2_FO,   '>', "$dada2_file"  || die " ERROR:\n Unable to write to <$dada2_file>\n";
open my $repseqs_FO, '>', "$repseq_file" || die " ERROR:\n Unable to write to <$repseq_file>\n";
my $feature_counter = 0;
$S->verbose("DADA2 Finished.");


# Dada2 taxonomy
if ($chooser->{'database'} eq 'dada2') {
	my $f = File::Spec->catfile("$temp_dir", 'taxonomy.tsv');
	$S->verbose("Converting " . $chooser->{'database'}.  " taxonomy output: " . $f);
	die " Taxonomy not found: $f\n" unless (-e $f);
  @taxonomy = loadTaxonomyArray($f);
	die " Error loading taxonomy: $f\n"if (scalar @taxonomy == 0);
}

# Create OTU Table and FASTA file from TSV
while ( my $line = readline($DADA_TSV) ) {
	if ($line=~/^#/) {
		$line=~s/_R1.fastq.gz//g;
		print {$dada2_FO} $line;
		chomp($line);
		@header = split /\t/, $line;
	} else {
		chomp($line);
		$feature_counter++;
		my ($sequence, @values) = split /\t/, $line;
		my $name;
    my $comment = '';
		if ($opt_seq_prefix eq 'MD5') {
			$name = md5_hex($sequence);
		} else {
			$name = $opt_seq_prefix . $feature_counter;
      $comment = "\t" . $taxonomy[$feature_counter - 1] if ($chooser->{'database'} eq 'dada2'); ##TODO
		}

		say {$repseqs_FO} '>' , $name, $comment, "\n", $sequence;
		say {$dada2_FO} $name, "\t", join("\t", @values);
	}

}

if ( defined $decipher_db ) {
	# Generate 'taxonomy.tsv' using the DECIPHER database
	
	$S->verbose("Assigning taxonomy using DECIPHER: " . $decipher_db);
	die  "$repseq_file?\n" if not -e "$repseq_file";
	my $decipher_cmd = qq($RealBin/dadaist2-assigntax -i "$repseq_file" -o "$temp_dir" -r "$decipher_db" -t "$opt_threads");
	

	$S->run($decipher_cmd);
}

if ($chooser->{'database'} eq 'decipher') {
	my $f = File::Spec->catfile("$temp_dir", 'taxonomy.tsv');
	$S->verbose("Converting " . $chooser->{'database'}.  " taxonomy output: " . $f);
	die " Taxonomy not found: $f\n" unless (-e $f);
  @taxonomy = loadTaxonomyArray($f);
	die " Error loading taxonomy: $f\n"if (scalar @taxonomy == 0);
}
my %output_files = (
 'feature-table' => "$dada2_file",
 'rep-seqs'      => "$repseq_file",
);


# Copy optional files
if ($opt_tax_db ne 'skip' and not defined $decipher_db) {
	# Taxonomy generated via DADA2
  $output_files{'dada-taxonomy-table'} = File::Spec->catfile("$opt_output_directory", 'taxonomy.txt');
	$S->verbose("Copying DADA2 taxonomy: " . File::Spec->catfile("$temp_dir", 'taxonomy.tsv')) if $opt_debug;
	copy(File::Spec->catfile("$temp_dir", 'taxonomy.tsv'),
	     File::Spec->catfile("$opt_output_directory", 'taxonomy.txt')) || 
			 die " ERROR:\n Unable to copy taxonomy.txt file from " . File::Spec->catfile("$temp_dir", 'taxonomy.txt') . " to $opt_output_directory.\n";
} elsif (defined $decipher_db) {
	# Taxonomy was generated via DECIPHER
	$output_files{'decipher-taxonomy-table'} = File::Spec->catfile("$opt_output_directory", 'taxonomy.txt');
	copy(File::Spec->catfile("$temp_dir", 'taxonomy.tsv'),
	     File::Spec->catfile("$opt_output_directory", 'taxonomy.txt')) || 
			 die " ERROR:\n Unable to copy taxonomy.txt file from " . File::Spec->catfile("$temp_dir", 'taxonomy.txt') . " to $opt_output_directory.\n";
}

if ($save_rds eq 'save') {
  $output_files{'feature-table-RDS'} = File::Spec->catfile("$opt_output_directory", 'feature-table.rds');
  copy(File::Spec->catfile("$dada2_temp", 'dada2.rds'),
	     File::Spec->catfile("$opt_output_directory", 'feature-table.rds')) || die " ERROR:\n Unable to copy RDS file from " . File::Spec->catfile("$temp_dir",  'dada2.rds') . " to $opt_output_directory.\n";

}

# Make Tree
my $msa_output  = File::Spec->catfile("$opt_output_directory", 'rep-seqs.msa');
my $tree_output = File::Spec->catfile("$opt_output_directory", 'rep-seqs.tree');
my $msa_cmd     = qq(clustalo -i "$repseq_file" -o "$msa_output" --outfmt=fasta --force);
my $tree_cmd    = qq(fasttree -nt -gtr -no2nd -spr 4 -quiet "$msa_output" > "$tree_output");
$S->verbose("Tree generation");
my $msa_exec    = $S->run($msa_cmd, { candie => 1 });
my $tree_exec   = $S->run($tree_cmd , { candie => 1 });
if ( $msa_exec->{'exit'} != 0 or $tree_exec->{'exit'} != 0) {
  say STDERR color('red'), " *** ERROR *** " if ($opt_verbose);
	$S->verbose("Clustalo execution failed.\n cmd> $msa_cmd\noutput> $msa_exec->{stderr}\nstderr> $msa_exec->{stdout}");
	exit 1;
} elsif ( $tree_exec->{'exit'} != 0) {
  say STDERR color('red'), " *** ERROR *** " if ($opt_verbose);
  $S->verbose("FastTree execution failed.\n cmd> $tree_cmd\noutput> $tree_exec->{stderr}\nstderr> $tree_exec->{stdout}");
	exit 1;
} else {
  $output_files{'multiple-alignment'} = $msa_output;
  $output_files{'features-tree'} = $tree_output;
}


# Delete, or not, temp dir
if ($opt_debug) {
  $S->verbose("Temporary directory _not_ deleted: $temp_dir");
} else {
  $S->verbose("Cleaning up");
  $S->run(qq(rm -rf "$temp_dir"));

}
# Export MicrobiomeAnalyst

my $export_cmd = qq($RealBin/dadaist2-exporter -i "$opt_output_directory");
my $exported = $S->run($export_cmd, { candie => 1});

if ($exported->{'exit'} == 0) {
  $output_files{'mba-files'} = File::Spec->catdir($opt_output_directory, 'MicrobiomeAnalyst');
} else {
	$S->verbose("MicrobiomeAnalyst not ready.");
}
# Print relevant output files produced
my $output_message = "Dadaist finished, output files saved:\n";
for my $f (keys %output_files) {
  $output_message .= " * $f: " . $output_files{$f} . "\n";
}
$S->verbose($output_message);

if (notify_message("Dadaist2 finished.")) {
	$S->run(qq(open "$opt_output_directory"), { candie => 1});
}

sub version {
	say "$PROGRAM v$VERSION";
	exit 0;
}

sub usage {
	say STDERR<<END;
USAGE:
 dadaist2 -i INPUT_DIR -o OUTPUT_DIR [-t TEMP_DIR]

 dadaist2 --help for full manual
END

 if ($_[0]) {
 	say "ERROR: $_[0]";
 	exit 1;
 }
}

sub check_metadata {
  my $file = shift @_;
  my $I;
  if (not open ($I, '<', $file)) {
    $S->verbose("Unable to load metadata from $file");
    exit 1;
  }
  my $count_lines = 0;
  my %samples = ();
  while (my $line = readline($I)) {
    $count_lines++;
    next if ($line =~/^#/);
    my @fields = split /$opt_metadata_separator/, $line;
    $samples{ $fields[0] }++;
  }
  return \%samples;
}

sub loadTaxonomyArray {
  # load taxonomy from files
  my $file = $_[0];
  my @results = ();
  return () if (! -e "$file");
  open(my $I, '<', "$file") || return ();
  my $c = 0;
  while (my $line = readline($I)) {
    chomp($line);
    $c++;
    next if ($c == 1);
    if ($line =~/^\d+/) {
      my @fields = split / /, $line;
      shift(@fields);
      push(@results, join(';', @fields));
    }
  }
  return @results;
}
sub get_qualified_positions {
	# Scan a list of qualityes and return the boundaries of qualified quality
	my $len = @_;
	my $start = undef;
	my $end = undef;

	for (my $pos = 0; $pos < ($len - $qualified_wnd); $pos++) {
		my $avg = avg_array( @_[$pos..$pos+$qualified_wnd] );

		if ($avg > $qualified_min_qual) {
			$start = $pos unless defined $start;
		} else {
			next if not defined $start;
			$end = $pos unless defined $end;
		}


	}
	$end = $len unless defined $end;

	return($start, $end, $len);
}
sub avg_array {
	my $agg = 0;
	$agg += $_ for @_;
	return $agg/@_;
}
sub sum_array {
	my $agg = 0;
	$agg += $_ for @_;
	return $agg
}

sub load_json_from_file {
	my $file = shift @_;
	my $json_read = $S->run(qq(cat "$file"));


	my $data;
	eval {
		$data = decode_json $json_read->{stdout};
	};
	if ($@) {
		die " FATAL ERROR: Unable to decode JSON from <$file>:\n$@\n";
	}
	return $data;

}
sub get_file_reads {
	my ($dir) = @_;
	my $is_paired = undef;
	$dir = File::Spec->rel2abs($dir);
	my @files = <"$dir"/*.*>;
	my %samples;
	my $counter_for = 0;
	my $counter_rev = 0;
	for my $file (sort @files) {
		next if (substr($file, 0, 1) eq '.');
		my ($id) = split /$opt_id_separator/, basename($file);

		if ( $samples{$id}{'for'} and $samples{$id}{'rev'} ) {
			$S->verbose("Error: Sample ID <$id> was already found. The delimiter ($opt_id_separator) is probably contained in the filename ID.");
			die "FATAL ERROR: Duplicateed SampleID <$id> in file $file.\n";
		}
		if ($file =~/$opt_for_tag/) {
			$counter_for++;
			$samples{$id}{'for'} = $file;
		} elsif ($file =~/$opt_rev_tag/) {
			$counter_rev++;
			$samples{$id}{'rev'} = $file;
		} else {
			$S->verbose("Skipping file <$file>: missing $opt_for_tag/$opt_rev_tag");
		}
	}
	if ($counter_for == 0 and $counter_rev == 0) {
		die "FATAL ERROR: No samples found in <$dir>.\n";
	} elsif ($counter_for == $counter_rev) {
		$is_paired = 1;
	} elsif ($counter_rev == 0 ) {
		$is_paired = 0;
	} else {
		die "FATAL ERROR: $counter_rev paired samples found, but $counter_for forward pairs found.\n";
	}
	return (\%samples, $is_paired, $counter_rev);
}

sub valid_gzipped_fasta {
	my $file = shift @_;
	my $reader;
	my $check = eval {
		$reader = FASTX::Reader->new({ filename => "$file"});

	};
	if ($@) {
		$S->verbose("Reference $file reading error: unable to open:\n$@");
		return 0;
	} else {
		if ($reader->{compressed} == 1) {
			return 1;
		} else {
			$S->verbose("Reference $file should be compressed with GZip!");
		}

	}

	exit;
}
sub check_required_binaries {

	for my $bin (sort keys  %{ $_[0]} ) {
		my $run = $S->run("${ $_[0]}{$bin}", { candie => 1});
		if ($run->{exit} == 0 and $run->{stdout} !~/not found/) {
			$S->verbose("Checking $bin: $run->{stdout}");
		} else {
			die "FATAL ERROR:\nDependency '$bin' not found or returned non zero exit value.\n";
		}

	}
}

sub prepare_temporary_directories {
 my $temp_dir = shift @_;
 $temp_dir = File::Spec->rel2abs($temp_dir);
	mkdir "$temp_dir/for"            || die " FATAL ERROR:\n Unable to create directory $temp_dir/for\n $!\n";
	mkdir "$temp_dir/rev"            || die " FATAL ERROR:\n Unable to create directory $temp_dir/rev\n $!\n";
	mkdir "$temp_dir/for/filtered"   || die " FATAL ERROR:\n Unable to create directory $temp_dir/for/filtered\n $!\n";
	mkdir "$temp_dir/rev/filtered"   || die " FATAL ERROR:\n Unable to create directory $temp_dir/rev/filtered\n $!\n";
	mkdir "$dada2_temp"              || die " FATAL ERROR:\n Unable to create directory $temp_dir/dada2\n $!\n";
  $S->verbose("Temporary directory: $temp_dir");
  return $temp_dir;
}

sub notify_message {
	my ($message) = @_;
	if ($opt_notify) {
		my $timestamp = getLoggingTime();
		my $cmd = "osascript -e \"display notification \\\"$message\\\" with title \\\"Dadaist2\\\" subtitle \\\"\\\" \" ";
		$cmd .= "|| zenity --notification --text \"DADAIST2: $message\" ";
		$cmd .= "|| notify-send \"DADAIST2: $message\"";
		my $i = $S->run($cmd,
		{candie => 1});

		if ($i->{'exit'} == 0) {
			return 1;
		} else {
			return 0;
		}
	} 
}

sub getLoggingTime {

    my ($sec,$min,$hour,$mday,$mon,$year,$wday,$yday,$isdst)=localtime(time);
    my $nice_timestamp = sprintf ( "%04d%02d%02d %02d:%02d:%02d",
                                   $year+1900,$mon+1,$mday,$hour,$min,$sec);
    return $nice_timestamp;
}
__END__

=pod

=head1 SOURCE CODE AND DOCUMENTATION

The program is freely available at L<https://quadram-institute-bioscience.github.io/dadaist2>
released under the MIT licence. The website contains further DOCUMENTATION.

=cut
